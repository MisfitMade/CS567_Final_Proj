{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from FP567_Lib import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We have all the files in resources/market_item_data/, which are item infos in the form of:\n",
    "{ \n",
    "    item_id : \n",
    "    [\n",
    "        [date/time_n+1, price_at_time_n+1, amount_sold_at_time_n+1], [date/time_n+2, price_at_time_n+2, amount_sold_at_time_n+2], ..., [date/time_n+m, price_at_time_n+m, amount_sold_at_time_n+m]\n",
    "    ]\n",
    "}\n",
    "for some amount of time.\n",
    "The amount of time varies between items, as not all items have existed as long as others.\n",
    "\n",
    "Let's make a Market object, which computes a bunch of the info we want.\n",
    "'''\n",
    "market = Market()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "So, lets extend all the items that are not highest_unit_time worth of info, backwards in time,\n",
    "so that the items that do not have as many as highest_unit_time, now have highest_unit_time\n",
    "amount of info, with the time stamp, but just 0, 0 for those days.\n",
    "\n",
    "To do that, we can call the balance method of the Market object, using the \n",
    "longest time span of unix times, and 0, 0 as the default amount sold and price\n",
    "'''\n",
    "market.balance_as_is(0, 0)\n",
    "market.is_balanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now we have a balanced market and want to include updates into a matrix in the below form.\n",
    "Notice how it is essentially m matrices, where each matrix represents a unit of time,\n",
    "appended onto one another from left to right, and is n rows by k+2 cols = amount_sold col + price col + k embedded update cols\n",
    "        | unix_time_0                                                                                                       | unix_time_1                                                                                                       |     | unix_time_m \n",
    "________| amount_sold_0 | price_0 | update_unix_time_0_feat_1 | update_unix_time_0_feat_2 | ... | update_unix_time_0_feat_k | amount_sold_1 | price_1 | update_unix_time_1_feat_1 | update_unix_time_1_feat_2 | ... | update_unix_time_1_feat_k | ... | amount_sold_m | price_m | update_unix_time_m_feat_1 | update_unix_time_m_feat_2 | ... | update_unix_time_m_feat_k |\n",
    "item_1  |               |         |                           |                           | ... |                           |               |         |                           |                           | ... |                           | ... |               |         |                           |                           | ... |                           |\n",
    "item_2  |               |         |                           |                           | ... |                           |               |         |                           |                           | ... |                           | ... |               |         |                           |                           | ... |                           |\n",
    ".       |       .           .                   .                       .                   ...               .                    .             .                .                            .                ...               .                           .           .                  .                           .                ...               .             | \n",
    ".       |       .           .                   .                       .                   ...               .                    .             .                .                            .                ...               .                           .           .                  .                           .                ...               .             | \n",
    ".       |       .           .                   .                       .                   ...               .                    .             .                .                            .                ...               .                           .           .                  .                           .                ...               .             | \n",
    "item_n  |               |         |                           |                           | ... |                           |               |         |                           |                           | ... |                           |     |               |         |                           |                           | ... |                           |\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Alot of them will be zeros.\n",
    "There will be zeros for\n",
    "    update_unix_time_i_feat_j for all j when there was no update for unix_time_i.\n",
    "    an item's amount_sold_i and price_i when that item was not being sold for unix_time_i.\n",
    "\n",
    "So, need to build a matrix of all the items info and tack on the embedded update.\n",
    "Lets build it so that it goes embedded update cols, amount sold col, price col, so that later,\n",
    "when we do forcasting and a day's worth of cols are forcasted, the price will be the last col\n",
    "of the output and thus, easier to quickly spot the forcasted price.\n",
    "'''\n",
    "market.build_features_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Save the features matrix so we dont have to keep making it.\n",
    "'''\n",
    "market.save_market_with_updates_rep_as_csv(PATH_TO_ASSEMBLED_FORCASTING_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train a forcasting model.\n",
    "'''\n",
    "df, forcasted_day_len = get_forcasting_market_df(get_a_random_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we define the length of days we want to forcast at a time, divy up the data into\n",
    "training and validation, standardize the data.\n",
    "'''\n",
    "total_days = df.shape[1]/forcasted_day_len\n",
    "if not total_days.is_integer() or df.shape[1] % int(total_days) != 0:\n",
    "    raise Exception(\n",
    "        \"The market matrix is malformed. Total days =\",\n",
    "         int(total_days),\n",
    "        \"and forcasted day length =\",\n",
    "        forcasted_day_len,\n",
    "        \"and total cols in the market =\", df.shape[1])\n",
    "\n",
    "num_training_days = int(total_days*0.8)\n",
    "num_validation_days = total_days - num_training_days\n",
    "num_training_cols = num_training_days*forcasted_day_len\n",
    "num_validation_cols = num_validation_days*forcasted_day_len\n",
    "\n",
    "# divide into training and validation\n",
    "training_days_df = df.loc[:, :num_training_cols]\n",
    "validation_days_df = df.loc[:, num_training_cols:num_training_cols + num_validation_cols]\n",
    "# Normalize the data and convert to numpys\n",
    "scaler = StandardScaler()\n",
    "training_days_mat = scaler.fit(training_days_df).fit_transform(training_days_df).astype(np.float16)\n",
    "validation_days_mat = scaler.fit(validation_days_df).fit_transform(validation_days_df).astype(np.float16)\n",
    "training_days_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Find a days at a time to predict value that divides evenly into both the validations\n",
    "and training days (hopefully)\n",
    "'''\n",
    "initial_window_size_to_days_to_predict_scale = 12\n",
    "days_at_a_time_to_predict = 7 # just do 4 if none found\n",
    "for i in range(3, 10):\n",
    "    if i % num_validation_days == i % num_training_days == 0:\n",
    "        days_at_a_time_to_predict = i\n",
    "        break\n",
    "\n",
    "'''\n",
    "We want to say a window_size worth of columns are equal to predict_size worth of columns,\n",
    "then slide the window predict_size columns and repeat over and over.\n",
    "'''\n",
    "window_size = (days_at_a_time_to_predict * initial_window_size_to_days_to_predict_scale) * forcasted_day_len\n",
    "predict_size = days_at_a_time_to_predict*forcasted_day_len\n",
    "tensor_shape = (training_days_mat.shape[0], window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make two tensor stacks:\n",
    "    X =\n",
    "        [\n",
    "            sparse_tensor(items_feats from time 0 to time n),\n",
    "            sparse_tensor(items_feats from time p to time n + p),\n",
    "            ...,\n",
    "            sparse_tensor(items_feats from time i*p to time n + i*p)\n",
    "        ]\n",
    "    Y =\n",
    "        [\n",
    "            sparse_tensor(items_feats from time n to time n + p),\n",
    "            sparse_tensor(items_feats from time (n + p) to time (n + p) + p),\n",
    "            ...,\n",
    "            sparse_tensor(items_feats from time (n + i*p) to time (n + i*p) + p)\n",
    "        ]\n",
    "where we will train our model that X[j] = Y[j]\n",
    "'''\n",
    "i = 0\n",
    "X_sparse_tensor_stack = []\n",
    "Y_sparse_tensor_stack = []\n",
    "while True:\n",
    "    window_start = predict_size * i\n",
    "    window_end = window_start + window_size\n",
    "    predict_window_end = window_end+predict_size\n",
    "    if predict_window_end > training_days_mat.shape[1]:\n",
    "        break\n",
    "\n",
    "    # take all the cols from the start of our time span, to current window end\n",
    "    X = training_days_mat[:, window_start:window_end]\n",
    "    # make an association that those columns will be equal to the next days_at_a_time_to_predict days worth of columns\n",
    "    Y = training_days_mat[:, window_end:predict_window_end]\n",
    "\n",
    "    X_tensor = make_numpy_mat_into_tf_sparse_tensor(X, make_practice_sparse=True)\n",
    "    Y_tensor = make_numpy_mat_into_tf_sparse_tensor(Y, make_practice_sparse=True)\n",
    "\n",
    "    if X_tensor is None or Y_tensor is None:\n",
    "        print(\"i =\", i)\n",
    "        print(\"X =\\n\", X)\n",
    "        print(\"Y =\\n\", Y)\n",
    "        break\n",
    "    # else\n",
    "    X_sparse_tensor_stack.append(X_tensor)\n",
    "    Y_sparse_tensor_stack.append(Y_tensor)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Do the same thing but for the validation data\n",
    "'''\n",
    "i = 0\n",
    "X_sparse_tensor_stack_validation = []\n",
    "Y_sparse_tensor_stack_validation = []\n",
    "while True:\n",
    "    window_start = predict_size * i\n",
    "    window_end = window_start + window_size\n",
    "    predict_window_end = window_end+predict_size\n",
    "    if predict_window_end > validation_days_mat.shape[1]:\n",
    "        break\n",
    "\n",
    "    # take all the cols from the start of our time span, to current window end\n",
    "    X = validation_days_mat[:, window_start:window_end]\n",
    "    # make an association that those columns will be equal to the next days_at_a_time_to_predict days worth of columns\n",
    "    Y = validation_days_mat[:, window_end:predict_window_end]\n",
    "\n",
    "    X_tensor = make_numpy_mat_into_tf_sparse_tensor(X, make_practice_sparse=True)\n",
    "    Y_tensor = make_numpy_mat_into_tf_sparse_tensor(Y, make_practice_sparse=True)\n",
    "\n",
    "    if X_tensor is None or Y_tensor is None:\n",
    "        print(\"i =\", i)\n",
    "        print(\"X =\\n\", X)\n",
    "        print(\"Y =\\n\", Y)\n",
    "        break\n",
    "    # else\n",
    "    X_sparse_tensor_stack_validation.append(X_tensor)\n",
    "    Y_sparse_tensor_stack_validation.append(Y_tensor)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_16 (LSTM)              (32, 3500, 2520)          50813280  \n",
      "                                                                 \n",
      " dropout (Dropout)           (32, 3500, 2520)          0         \n",
      "                                                                 \n",
      " lstm_17 (LSTM)              (32, 3500, 1260)          19056240  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (32, 3500, 1260)          0         \n",
      "                                                                 \n",
      " lstm_18 (LSTM)              (32, 3500, 630)           4765320   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (32, 3500, 630)           0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (32, 3500, 210)          132510    \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 74,767,350\n",
      "Trainable params: 74,767,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model = get_model(tensor_shape, batch_size, predict_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a unique folder for this model and set up check points\n",
    "checkpoint_dir_x = os.path.join(\n",
    "    PATH_TO_MODELS_DIRECTORY,\n",
    "    datetime.now().strftime(\"%m/%d/%H:%M:%S\"))\n",
    "pathlib.Path(checkpoint_dir_x).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "call_backs = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        os.path.join(checkpoint_dir_x, \"save_at_{epoch}.h5\"),\n",
    "        save_weights_only=True),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10), # stop if the validation loss goes up for given number of epochs\n",
    "]\n",
    "\n",
    "# Train!\n",
    "epochs = 30\n",
    "history = model.fit(\n",
    "    X_sparse_tensor_stack,\n",
    "    Y_sparse_tensor_stack,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_sparse_tensor_stack_validation, Y_sparse_tensor_stack_validation),\n",
    "    verbose=1, # 0 is silent, 1 for loading bar, 2 for stats each epoch\n",
    "    callbacks=call_backs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model summary, plus it's hyper params, epoch count, optimizer type, etc to a json to be\n",
    "# reloaded later.\n",
    "# Plot the model's change in training and validation accuracy and loss over epochs.\n",
    "# Plot a confusion matrix using the validation data.\n",
    "print_model_summary_to_file(model, checkpoint_dir_x)\n",
    "save_training_params(\n",
    "    batch_size,\n",
    "    training_days_mat.shape[0],\n",
    "    days_at_a_time_to_predict,\n",
    "    initial_window_size_to_days_to_predict_scale,\n",
    "    predict_size,\n",
    "    window_size,\n",
    "    checkpoint_dir_x)\n",
    "plot_accuracy(plt, history, checkpoint_dir_x)\n",
    "plot_loss(plt, history, checkpoint_dir_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('567FP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8737e8cb70a0f7f021fa9834a2ae2129f817e796f624513bd53dcfd8a7bc265"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
